{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import hashlib, random\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import copy\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import jsonpickle\n",
    "from random import shuffle\n",
    "from collections import defaultdict\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import random, json\n",
    "from tqdm import trange, tqdm\n",
    "from operator import itemgetter\n",
    "from analyzer import *\n",
    "import query_gen\n",
    "\n",
    "sns.set(style='ticks', palette='coolwarm') #this overwrites matplotlib setting so make it run first \n",
    "\n",
    "# def set_font():\n",
    "import matplotlib\n",
    "# http://ishxiao.com/blog/python/2017/07/23/how-to-change-the-font-size-on-a-matplotlib-plot.html\n",
    "\n",
    "font = {'weight' : 'bold',\n",
    "#         'size'   : 20,\n",
    "       'family': 'Linux Libertine O'}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ratio(results):\n",
    "    cnt_mean_ratios = {}\n",
    "    sum_mean_ratios = {}\n",
    "    cnt_median_ratios = {}\n",
    "    sum_median_ratios = {}\n",
    "    cnt_ratios = {}\n",
    "    sum_ratios = {}\n",
    "    gt = results['GT']\n",
    "    for name, result in results.items():\n",
    "        if name == 'GT':\n",
    "            continue\n",
    "#         print(name)\n",
    "#         print(gt)\n",
    "#         print(result)\n",
    "        cnt_ratio = [float(b[0])/float(g[0]) for b, g in zip(result, gt)]\n",
    "        cnt_mean_ratios[name] = np.mean(cnt_ratio)\n",
    "        cnt_median_ratios[name] = np.median(cnt_ratio)\n",
    "#         print(pd.DataFrame(cnt_ratio).describe())\n",
    "        sum_ratio = [float(b[1])/float(g[1]) for b, g in zip(result, gt)]\n",
    "        sum_mean_ratios[name] = np.mean(sum_ratio)\n",
    "        sum_median_ratios[name] = np.median(sum_ratio)\n",
    "        sum_ratios[name] = sum_ratio\n",
    "        cnt_ratios[name] = cnt_ratio\n",
    "    return cnt_ratios, sum_ratios, cnt_mean_ratios, cnt_median_ratios, sum_mean_ratios, sum_median_ratios\n",
    "\n",
    "def plot_columns(mean, median, ylabel='Bound/GroundTruth Ratio', \n",
    "                 xlabel='Mean and Median', bar_labels=['Mean Ratio', 'Median Ratio'],\n",
    "                log_scale = True, yrange= None):\n",
    "    patterns = [ \"/\" , \"\\\\\" , \"|\" , \"-\" , \"+\" , \"x\", \"o\", \"O\", \".\", \"*\" ]\n",
    "    baselines = mean.keys()\n",
    "#     print(baselines)\n",
    "    if yrange is None:\n",
    "        yrange = [0, max(max(mean.values()), max(median.values())) + 1]\n",
    "    fig, ax = plt.subplots(1,1,figsize=(18,8))\n",
    "    \n",
    "    xfont = {'weight' : 'bold',\n",
    "            'size'   : 22}\n",
    "\n",
    "    width = 0.3\n",
    "    \n",
    "    mean_bar = [perf for perf in mean.values()]\n",
    "    median_bar = [perf for perf in median.values()]\n",
    "\n",
    "    r0 = np.arange(len(mean_bar))\n",
    "    r1 = [x - 0.15 for x in r0]\n",
    "    r2 = [x + width for x in r1]\n",
    "#     print(mean_bar, median_bar)\n",
    "    ax.bar(r1, mean_bar, width, edgecolor='black', label=bar_labels[0], hatch=patterns[0])\n",
    "    ax.bar(r2, median_bar, width, edgecolor='black', label=bar_labels[1], hatch=patterns[1])\n",
    "    ax.set_xticks(np.arange(len(baselines)))\n",
    "    ax.set_xticklabels(baselines, fontsize='x-large')\n",
    "    \n",
    "    ax.hlines(1.0, -0.5, len(baselines)-0.5, linestyle='--', color='gray', linewidth=4, label='Ground Truth')\n",
    "    \n",
    "    ax.set_ylabel(ylabel, fontdict = xfont)\n",
    "    \n",
    "    ax.set_ylim(yrange)\n",
    "    \n",
    "    if log_scale:\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "    ax.legend()\n",
    "    \n",
    "#     matplotlib.pyplot.sca(ax)\n",
    "    print(yrange)\n",
    "\n",
    "    plt.legend(fontsize=20, loc='upper left')\n",
    "    ax.set_xlabel(xlabel, fontdict = xfont)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "def plot_box(cnt_ratios, sum_ratios, ylabel='Bound/GroundTruth Ratio', xlabel='Mean and Median', bar_labels=['COUNT(*)', 'SUM(Light)']):\n",
    "    patterns = [ \"/\" , \"\\\\\" , \"|\" , \"-\" , \"+\" , \"x\", \"o\", \"O\", \".\", \"*\" ]\n",
    "    baselines = cnt_ratios.keys()\n",
    "#     print(max([max(r) for r in cnt_ratios.values()]), max(cnt_ratios.values()))\n",
    "#     yrange = [0, max(max([max(r) for r in cnt_ratios.values()]), max([max(r) for r in sum_ratios.values()])) + 1]\n",
    "    fig, ax = plt.subplots(1,1,figsize=(18,8))\n",
    "    xfont = {'weight' : 'bold',\n",
    "            'size'   : 22}\n",
    "\n",
    "    width = 0.5\n",
    "    \n",
    "    cnt_box = [perf for perf in cnt_ratios.values()]\n",
    "    sum_box = [perf for perf in sum_ratios.values()]\n",
    "\n",
    "    r0 = np.arange(len(cnt_ratios))\n",
    "    r1 = [x - 0.15 for x in r0]\n",
    "    r2 = [x + width for x in r1]\n",
    "#     print(mean_bar, median_bar)\n",
    "    ax.boxplot(cnt_box, positions = r1, showfliers=False)\n",
    "    ax.boxplot(sum_box, positions = r2, showfliers=False)\n",
    "    ax.set_xticks(np.arange(len(baselines)+1))\n",
    "    ax.set_xticklabels(baselines, fontsize='x-large')\n",
    "\n",
    "    ax.hlines(1.0, -1, len(baselines)+1.5, linestyle='--', color='gray', linewidth=2, label='Ground Truth')\n",
    "\n",
    "    ax.set_ylabel(ylabel, fontdict = xfont)\n",
    "    ax.set_xlim([-1, len(baselines)+1])\n",
    "#     ax.set_ylim(yrange)\n",
    "#         ax.set_yscale(\"log\")\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    #     matplotlib.pyplot.sca(ax)\n",
    "\n",
    "#         plt.legend(fontsize=20, loc='upper left')\n",
    "#     ax.set_xlabel(xlabel, fontdict = xfont)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def restore_obj(path):\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            s = f.read()\n",
    "            data = jsonpickle.decode(s, keys = True)\n",
    "            print(\"Loaded %s\" % path)\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(\"failed to restore\", path, e)\n",
    "        return None\n",
    "\n",
    "def load_and_merge(config):\n",
    "    # load all the files and merge them together because we have different files representing different partition type.\n",
    "    baseline_perf = defaultdict(dict)\n",
    "    queries = None\n",
    "    for name, path in config.items():\n",
    "        path = '/local/xi/VarAcc/src/cache/'+path\n",
    "        if 'json' in path:\n",
    "            data = json.load(open(path))\n",
    "        else:\n",
    "            data = restore_obj(path)\n",
    "        if name == 'GT':\n",
    "            queries = data\n",
    "        else:\n",
    "            k = name.split('-')[0]\n",
    "            for base, perf in data.items():\n",
    "                baseline_perf[k][base] = perf\n",
    "    return queries, baseline_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vulture\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /local/xi/VarAcc/src/cache/IntelWireless-RangeSumQuery-itime-light-2000.jpk\n",
      "CNT\n",
      "PASS 0.000992317698090024\n",
      "US 0.009448071478984263\n",
      "ST 0.0016109560952192372\n",
      "AQP++ 0.003353882644568703\n",
      "SUM\n",
      "PASS 0.0007895222109930471\n",
      "US 0.016145269030771042\n",
      "ST 0.010484856069597325\n",
      "AQP++ 0.005018299861210685\n",
      "AVG\n",
      "PASS 0.0012000180121661311\n",
      "US 0.012243917644156867\n",
      "ST 0.010524939740060522\n",
      "AQP++ 0.004033928800972551\n",
      "MIN\n",
      "PASS 0.0\n",
      "US 522.2189796547437\n",
      "ST 522.2189796547437\n",
      "AQP++ 0.0\n",
      "MAX\n",
      "PASS 0.0\n",
      "US 0.0\n",
      "ST 0.0\n",
      "AQP++ 0.0\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"IntelWireless\"\n",
    "\n",
    "n_queries = {'random':2000}\n",
    "gts = {'random':'IntelWireless-RangeSumQuery-itime-light-2000.jpk'}\n",
    " \n",
    "k_depths =[\n",
    "          [7,64]\n",
    "        ]\n",
    "\n",
    "baselines=[\n",
    "        {'alias':\"PASS\", 'name':\"DP_AMAX\"}, \n",
    "        {'alias':\"US\", 'name':\"EqualDepthPartitioner\"}, \n",
    "        {'alias':\"ST\", 'name':\"EqualDepthPartitioner\"}, \n",
    "        {'alias':\"AQP++\", \"name\":'HillClimbing'},\n",
    "     ]\n",
    "\n",
    "name_temp = \"{sr}-{alias}\"\n",
    "path_temp = \"{ds}-{name}-{alias}-{depth}-{k}-{sr}-{n_query}-{qname}.json\"\n",
    "\n",
    "config={}\n",
    "\n",
    "srs=[0.005]\n",
    "\n",
    "k_depths =[\n",
    "    \n",
    "    [7,64], \n",
    "    \n",
    "        ]\n",
    "\n",
    "name_temp = \"{k}-{alias}\"\n",
    "\n",
    "\n",
    "for workload, n_query in n_queries.items():\n",
    "    workload_config={}\n",
    "    workload_config['GT']=gts[workload]\n",
    "    for kd in k_depths:\n",
    "        k=kd[1]\n",
    "        depth=kd[0]\n",
    "        for sr in srs:\n",
    "            for baseline in baselines:\n",
    "                    alias=baseline['alias']\n",
    "                    name=baseline['name']\n",
    "                    baseline_name = name_temp.format(k=k, alias=alias)\n",
    "                    baseline_path = path_temp.format(ds=ds_name, name=name, alias=alias, depth=depth, k=k, sr=sr,n_query=n_query, qname=workload)\n",
    "                    workload_config[baseline_name] = baseline_path\n",
    "    config[workload]=workload_config\n",
    "\n",
    "\n",
    "queries, bp = load_and_merge(config['random'])\n",
    "random_analyzed = defaultdict(dict)\n",
    "for k in bp.keys():\n",
    "    # SUM query only\n",
    "    RE_results = RelativeError(queries, bp[k]).analyze()\n",
    "    FR_results = FailureRate(queries, bp[k]).analyze()\n",
    "    CI_results = PERCENTILE_CI(queries, bp[k]).analyze()\n",
    "    CIR_results = CIRatio(queries, bp[k]).analyze()\n",
    "    query_type = 1 #0 for cnt, 1 for sum, 2 for avg\n",
    "    random_analyzed[k]={\"Relative Error\": RE_results[query_type], \n",
    "                        \"Failure Rate\": FR_results[query_type], \n",
    "                        \"Half CI\": CI_results[query_type], \"CI Ratio\": CIR_results[query_type]\n",
    "                       }\n",
    "\n",
    "by_q_type  = {}\n",
    "for qname, query_type in {\"CNT\": 0, \"SUM\": 1, \"AVG\": 2, \"MIN\": 3, \"MAX\": 4}.items():\n",
    "    for k in bp.keys():\n",
    "        # SUM query only\n",
    "        RE_results = RE_w_MinMax(queries, bp[k]).analyze()\n",
    "#         FR_results = FailureRate(queries, bp[k]).analyze()\n",
    "#         CI_results = PERCENTILE_CI(queries, bp[k]).analyze()\n",
    "#         CIR_results = CIRatio(queries, bp[k]).analyze()\n",
    "        by_q_type[k]={\"Relative Error\": RE_results[query_type], \n",
    "#                             \"Failure Rate\": FR_results[query_type], \n",
    "#                             \"Half CI\": CI_results[query_type], \"CI Ratio\": CIR_results[query_type]\n",
    "                           }\n",
    "    print(qname)\n",
    "    for base in ['PASS', 'US', 'ST', 'AQP++']:\n",
    "        print(base, np.percentile(by_q_type['64']['Relative Error'][base], 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /local/xi/VarAcc/src/cache/Instacart-RangeSumQuery-product_id-reordered-2000.jpk\n",
      "CNT\n",
      "PASS 0.0003697714051830369\n",
      "US 0.012057285011092708\n",
      "ST 0.0022665251770069856\n",
      "AQP++ 0.0037569305156833713\n",
      "SUM\n",
      "PASS 0.0006300513150947784\n",
      "US 0.018237932228759454\n",
      "ST 0.01277132521037442\n",
      "AQP++ 0.004738916755839947\n",
      "AVG\n",
      "PASS 0.000534980015427974\n",
      "US 0.01254999638680509\n",
      "ST 0.012258833351829828\n",
      "AQP++ 0.003053937363282077\n",
      "MIN\n",
      "PASS 0.0\n",
      "US 0.0\n",
      "ST 0.0\n",
      "AQP++ 0.0\n",
      "MAX\n",
      "PASS 0.0\n",
      "US 0.0\n",
      "ST 0.0\n",
      "AQP++ 0.0\n"
     ]
    }
   ],
   "source": [
    "ds_name = \"Instacart\"\n",
    "\n",
    "n_queries = {'random':2000}\n",
    "gts = {'random':'Instacart-RangeSumQuery-product_id-reordered-2000.jpk'}\n",
    "     \n",
    "baselines=[\n",
    "        {'alias':\"PASS\", 'name':\"DP_AMAX\"}, \n",
    "        {'alias':\"US\", 'name':\"EqualDepthPartitioner\"}, \n",
    "        {'alias':\"ST\", 'name':\"EqualDepthPartitioner\"}, \n",
    "        {'alias':\"AQP++\", \"name\":'HillClimbing'},\n",
    "     ]\n",
    "config={}\n",
    "\n",
    "srs=[0.005]\n",
    "\n",
    "k_depths =[\n",
    "    \n",
    "    [7,64], \n",
    "    \n",
    "        ]\n",
    "\n",
    "name_temp = \"{k}-{alias}\"\n",
    "path_temp = \"{ds}-{name}-{alias}-{depth}-{k}-{sr}-{n_query}-{qname}.json\"\n",
    "\n",
    "for workload, n_query in n_queries.items():\n",
    "    workload_config={}\n",
    "    workload_config['GT']=gts[workload]\n",
    "    for kd in k_depths:\n",
    "        k=kd[1]\n",
    "        depth=kd[0]\n",
    "        for sr in srs:\n",
    "            for baseline in baselines:\n",
    "                    alias=baseline['alias']\n",
    "                    name=baseline['name']\n",
    "                    baseline_name = name_temp.format(k=k, alias=alias)\n",
    "                    baseline_path = path_temp.format(ds=ds_name, name=name, alias=alias, depth=depth, k=k, sr=sr,n_query=n_query, qname=workload)\n",
    "                    workload_config[baseline_name] = baseline_path\n",
    "    config[workload]=workload_config\n",
    "\n",
    "\n",
    "queries, bp = load_and_merge(config['random'])\n",
    "random_analyzed = defaultdict(dict)\n",
    "for k in bp.keys():\n",
    "    # SUM query only\n",
    "    RE_results = RelativeError(queries, bp[k]).analyze()\n",
    "    FR_results = FailureRate(queries, bp[k]).analyze()\n",
    "    CI_results = PERCENTILE_CI(queries, bp[k]).analyze()\n",
    "    CIR_results = CIRatio(queries, bp[k]).analyze()\n",
    "    query_type = 1 #0 for cnt, 1 for sum, 2 for avg\n",
    "    random_analyzed[k]={\"Relative Error\": RE_results[query_type], \n",
    "                        \"Failure Rate\": FR_results[query_type], \n",
    "                        \"Half CI\": CI_results[query_type], \"CI Ratio\": CIR_results[query_type]\n",
    "                       }\n",
    "\n",
    "by_q_type  = {}\n",
    "for qname, query_type in {\"CNT\": 0, \"SUM\": 1, \"AVG\": 2, \"MIN\": 3, \"MAX\": 4}.items():\n",
    "    for k in bp.keys():\n",
    "        # SUM query only\n",
    "        RE_results = RE_w_MinMax(queries, bp[k]).analyze()\n",
    "#         FR_results = FailureRate(queries, bp[k]).analyze()\n",
    "#         CI_results = PERCENTILE_CI(queries, bp[k]).analyze()\n",
    "#         CIR_results = CIRatio(queries, bp[k]).analyze()\n",
    "        by_q_type[k]={\"Relative Error\": RE_results[query_type], \n",
    "#                             \"Failure Rate\": FR_results[query_type], \n",
    "#                             \"Half CI\": CI_results[query_type], \"CI Ratio\": CIR_results[query_type]\n",
    "                           }\n",
    "    print(qname)\n",
    "    for base in ['PASS', 'US', 'ST', 'AQP++']:\n",
    "        print(base, np.percentile(by_q_type['64']['Relative Error'][base], 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /local/xi/VarAcc/src/cache/Taxi-RangeSumQuery-pickup_datetime-trip_distance-2000.jpk\n",
      "CNT\n",
      "PASS 0.00023983516212723453\n",
      "US 0.005080087997217773\n",
      "ST 0.0008757561812368392\n",
      "AQP++ 0.0016471239849696106\n",
      "SUM\n",
      "PASS 0.0005618153333024449\n",
      "US 0.010002918915751634\n",
      "ST 0.008876689228049264\n",
      "AQP++ 0.0027018772314121066\n",
      "AVG\n",
      "PASS 0.0005245173090414003\n",
      "US 0.0087711064083602\n",
      "ST 0.008904518018372169\n",
      "AQP++ 0.0022275934062794918\n",
      "MIN\n",
      "PASS 0.0\n",
      "US 0.0\n",
      "ST 0.0\n",
      "AQP++ 0.0\n",
      "MAX\n",
      "PASS 0.0\n",
      "US 0.9030540887683745\n",
      "ST 0.9047367155566242\n",
      "AQP++ 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds_name = \"Taxi\"\n",
    "\n",
    "n_queries = {'random':2000}\n",
    "gts = {'random':'Taxi-RangeSumQuery-pickup_datetime-trip_distance-2000.jpk'}\n",
    "          \n",
    "baselines=[\n",
    "        {'alias':\"PASS\", 'name':\"DP_AMAX\"}, \n",
    "        {'alias':\"US\", 'name':\"EqualDepthPartitioner\"}, \n",
    "        {'alias':\"ST\", 'name':\"EqualDepthPartitioner\"}, \n",
    "        {'alias':\"AQP++\", \"name\":'HillClimbing'},\n",
    "     ]\n",
    "config={}\n",
    "\n",
    "srs=[0.005]\n",
    "\n",
    "k_depths =[\n",
    "    \n",
    "    [7,64], \n",
    "    \n",
    "        ]\n",
    "\n",
    "name_temp = \"{k}-{alias}\"\n",
    "path_temp = \"{ds}-{name}-{alias}-{depth}-{k}-{sr}-{n_query}-{qname}.json\"\n",
    "\n",
    "for workload, n_query in n_queries.items():\n",
    "    workload_config={}\n",
    "    workload_config['GT']=gts[workload]\n",
    "    for kd in k_depths:\n",
    "        k=kd[1]\n",
    "        depth=kd[0]\n",
    "        for sr in srs:\n",
    "            for baseline in baselines:\n",
    "                    alias=baseline['alias']\n",
    "                    name=baseline['name']\n",
    "                    baseline_name = name_temp.format(k=k, alias=alias)\n",
    "                    baseline_path = path_temp.format(ds=ds_name, name=name, alias=alias, depth=depth, k=k, sr=sr,n_query=n_query, qname=workload)\n",
    "                    workload_config[baseline_name] = baseline_path\n",
    "    config[workload]=workload_config\n",
    "\n",
    "\n",
    "queries, bp = load_and_merge(config['random'])\n",
    "random_analyzed = defaultdict(dict)\n",
    "for k in bp.keys():\n",
    "    # SUM query only\n",
    "    RE_results = RelativeError(queries, bp[k]).analyze()\n",
    "    FR_results = FailureRate(queries, bp[k]).analyze()\n",
    "    CI_results = PERCENTILE_CI(queries, bp[k]).analyze()\n",
    "    CIR_results = CIRatio(queries, bp[k]).analyze()\n",
    "    query_type = 1 #0 for cnt, 1 for sum, 2 for avg\n",
    "    random_analyzed[k]={\"Relative Error\": RE_results[query_type], \n",
    "                        \"Failure Rate\": FR_results[query_type], \n",
    "                        \"Half CI\": CI_results[query_type], \"CI Ratio\": CIR_results[query_type]\n",
    "                       }\n",
    "\n",
    "by_q_type  = {}\n",
    "for qname, query_type in {\"CNT\": 0, \"SUM\": 1, \"AVG\": 2, \"MIN\": 3, \"MAX\": 4}.items():\n",
    "    for k in bp.keys():\n",
    "        # SUM query only\n",
    "        RE_results = RE_w_MinMax(queries, bp[k]).analyze()\n",
    "#         FR_results = FailureRate(queries, bp[k]).analyze()\n",
    "#         CI_results = PERCENTILE_CI(queries, bp[k]).analyze()\n",
    "#         CIR_results = CIRatio(queries, bp[k]).analyze()\n",
    "        by_q_type[k]={\"Relative Error\": RE_results[query_type], \n",
    "#                             \"Failure Rate\": FR_results[query_type], \n",
    "#                             \"Half CI\": CI_results[query_type], \"CI Ratio\": CIR_results[query_type]\n",
    "                           }\n",
    "    print(qname)\n",
    "    for base in ['PASS', 'US', 'ST', 'AQP++']:\n",
    "        print(base, np.percentile(by_q_type['64']['Relative Error'][base], 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
